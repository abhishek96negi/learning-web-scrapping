{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab1442eb",
   "metadata": {},
   "source": [
    "# Web Scraping with Python\n",
    "\n",
    "## HTML PAGE\n",
    "\n",
    "* Simplist method for basic pages\n",
    "\n",
    "* Request to get page\n",
    "\n",
    "* BeautifulSoup to parse the HTML data\n",
    "\n",
    "* View Source\n",
    "\n",
    "* HTML Tables\n",
    "\n",
    "* Pagination by changing URLs\n",
    "\n",
    "\n",
    "## Dealing with Logins\n",
    "\n",
    "* Useful for business sustems\n",
    "\n",
    "* Inspect element -> Network tab\n",
    "\n",
    "* Login manually look to where the login data is being sent.\n",
    "\n",
    "* Replicate with Requests in Python.\n",
    "\n",
    "* Use Session () to remain logged in\n",
    "\n",
    "## Rendering JavaScript\n",
    "\n",
    "* Modern websites use JS to dynamically load data.\n",
    "\n",
    "* Scripts must be loaded by a browser\n",
    "\n",
    "* Requests-html and Render() runs Chromium in the backround to exec scripts\n",
    "\n",
    "* Has a HTML parser we can use to collect data.\n",
    "\n",
    "## API Endpoints\n",
    "\n",
    "* Requires more investigation\n",
    "\n",
    "* Skip out rendering the page data and get it from the source.\n",
    "\n",
    "* Network tab -> XHR find JSON data\n",
    "\n",
    "* Replicate and change in API program like Postman or Insomnia\n",
    "\n",
    "* Download with requests and JSON module\n",
    "\n",
    "## Selenium, Browser Automation and Helium\n",
    "\n",
    "* Use selenium control the browser.\n",
    "\n",
    "* Not ideal solution, slow and resource heavy.\n",
    "\n",
    "* Can be used to click buttons and write in text boxes.\n",
    "\n",
    "* Get data from Elements or download rendered html source.\n",
    "\n",
    "* Scroll and load dynamic Elements\n",
    "\n",
    "## Legality and Other Notes\n",
    "\n",
    "* Can be a grey area.\n",
    "\n",
    "* Scrape only publically available data.\n",
    "\n",
    "* Check your countries laws.\n",
    "\n",
    "* Obey robots.txt\n",
    "\n",
    "* Anti bot measures\n",
    "\n",
    "* Don't spam requests to other peoples servers \n",
    "\n",
    "* Use APIs where available\n",
    "\n",
    "## Web Scraping Tips 1\n",
    "\n",
    "**INVESTIGATE**\n",
    "\n",
    "* N : - The Network Tab: This shows all the network activity from the server to the browser and often contains the API endpoint for easy access\n",
    "\n",
    "* I :- Inspect Element: The DOM and Inspect Element tools show what the browser is interpreting and is very useful for finding the elements that contain the data we want.\n",
    "\n",
    "* S :- View Source: Check what your http request is actually seeing, this will give you a good idea of how you need to proceed.\n",
    "\n",
    "* H :- HTML: Plan HTML still exists a lot even on the modern web, check it out it might be the best option for data extraction.\n",
    "\n",
    "## Web Scraping Tips 2\n",
    "\n",
    "**PARSE LOCALLY**\n",
    "\n",
    "Saving a copy of the HTML or JSON data you are working on to you local hard drive with help you speed up parsing and data extraction without sending multiple requests to the server.\n",
    "\n",
    "## Web Scraping Tips 3\n",
    "\n",
    "**WRITE A PLAN**\n",
    "\n",
    "Using pseudocode or writing out a plan/scope for your project will help you keep on track with what you are trying to achieve, and not let you get carried away spending time on things that arenâ€™t core to the project.\n",
    "\n",
    "## Web Scraping Tips 4\n",
    "\n",
    "**KEEP IT SIMPLE**\n",
    "\n",
    "Keep feature to a minimum for now. Orangize you files and keep the scraper doing just that, scrapping data. Save the analysis for another PY file. Write clear and concise functions and classes where necessary.\n",
    "\n",
    "## Web Scrapping Tips 5\n",
    "\n",
    "**THE RIGHT TOOL**\n",
    "\n",
    "Rendering the page or using Selenium should be the last resort! There is almost always a better way - if you did tip 1, investigate the site properly you may have found the API endpoint, or JSON in the HTML somewhere."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
